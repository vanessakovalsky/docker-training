# Lab 7 : Administer containers by centralizing the logs and setting up monitoring

This TP aims to set up a response to two need: centralize and analyze the logs on the one hand and monitor our Docker containers on the other.
We will use the Stack Elk for the first need, and a suite composed of Cadvisor, Promotheus and Grafana for the need for monitoring.

## Log's management

### Establishment of an ELK application
- We recover the code here: https://github.com/deviantony/docker-elk.git (Git Clone https://github.com/deviantony/docker-elk.git)
- Docker- compose.yml is used at the root
- It only remains to launch it with a docker-compose up
- Once the start is finished, check by going to:
http://ip_machine:9200/status which displays Elastic Search
- Kibana is accessible to the address: http: // ip_machine: 5601 but for the moment without given it is not displayed
- The environment is ready, it must now be supplied with data

### Send data to Elk
- To feed ELK in data, we use FileBeats which will automatically read the logs generated by Docker, and send them to Logsh
- To do this, add to Docker compose the Filebeat service :
```
   filebeat:
    image: docker.elastic.co/beats/filebeat:8.6.2
    command: filebeat -e -strict.perms=false
    #volumnes mount depend on you OS ( Windows or Linux )
    volumes:
      - ./filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    user: root
    networks:
      - elk
    links:
       - elasticsearch
       - kibana
```
- Then create a filebeat folder and a filebeat.yml file inside this folder which contains :
```
filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true

filebeat.inputs:
- type: container
  paths:
  - '/var/lib/docker/containers/*/*.log'

processors:
- add_cloud_metadata: ~

output.elasticsearch:
  hosts: ['elasticsearch:9200']
  username: 'elastic'
  password: 'changeme'
```
- To verify that it works, return to Kibana's page, and validate the index creation form (this indicates that Kibana has received data)
- Look for in Kibana's interface or are the logs of our container?
- Relaunch the Docker-compose of lab 5 and go and check that the logs appear well in Kibana


## Implementation of monitoring
### Add a supervision agent to our application

- Start with Docker composed of lab 5 containing our application, and adding a Cadvisor supervision agent

- Let's create a new docker-compose.yml to add a Cadvisor service, which will be responsible for recovering the containers' metrics :
```
  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    expose:
      - 8080
    ports:
      - "8005:8080"
    networks:
      - monitoring

networks:
  monitoring:
    driver: bridge
```
- Remember to add the monitoring network on other containers
- Start the container (Docker-compose up)
- Going on: http://ip_machine:8005 you can view the metrics of your containers

### Adding promotheus to make our supervision configurable
- To go further and make this configuration, add Promotheus to Docker-Concomose :
```
prometheus:
      image: prom/prometheus:latest
      container_name: prometheus
      volumes:
        - ./docker/prometheus/:/etc/prometheus/
        - prometheus-data:/prometheus
      command:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--storage.tsdb.retention=200h'
      expose:
        - 9090
      ports:
        - "9090:9090"
      networks:
        - monitoring

volumes:
...
  prometheus-data: {}
```
- Create a folder /Prometheus and inside this folder a Prometheus.yml file (which is used for configuration) with the following content :
```
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'docker-host-alpha'

rule_files:
  - "targets.rules"
  - "host.rules"
  - "containers.rules"

scrape_configs:
  - job_name: 'cadvisor'
    scrape_interval: 5s
    static_configs:
      - targets: ['cadvisor:8080']

  - job_name: 'prometheus'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:9090']
```
- Relaunch a down a up on the Docker-Concomose
- prometheusEstAccessibleSur :Http://ipMachine:9090/targets
- We can then check the status of our containers

### Add dashboard with grafana

- Then add the interface to create and view Dashboard Grafana at Docker composes :
```
 grafana:
    image: grafana/grafana:latest
    container_name: grafana
    expose:
      - 3000
    ports:
      - "3000:3000"
    networks:
      - monitoring
```
- Then rebuild and relaunch the Docker composes
- Grafana is accessible on http: // ip_machine: 3000 (login and default password: admin)
- It only remains to configure the data source (promotheuses) and to create dashboard (for example you can import the dashboard https://grafana.com/grafana/dashboards/193 planned for docker)
For the URL Put the container IP with Promotheus with Port 9090 (to find it Docker Network Inspect Monitoring)

-> Congratulations Your applications is now monitoring and its centralized logs, it is almost ready for production :)
